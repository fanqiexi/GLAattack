{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a0757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  9 18:10:49 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.31       Driver Version: 465.31       CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\r\n",
      "| 34%   28C    P8    24W / 350W |      0MiB / 24268MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c612dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from modules.tokenization_clip import SimpleTokenizer as ClipTokenizer\n",
    "from modules.modeling import CLIP2Video\n",
    "from evaluation.eval import eval_epoch\n",
    "\n",
    "from utils.config import get_args\n",
    "from utils.utils import get_logger\n",
    "from utils.dataloader import dataloader_msrvtt_train\n",
    "from utils.dataloader import dataloader_msrvtt_test\n",
    "from utils.dataloader import dataloader_msrvttfull_test\n",
    "from utils.dataloader import dataloader_msvd_train\n",
    "from utils.dataloader import dataloader_msvd_test\n",
    "from utils.dataloader import dataloader_vatexEnglish_train\n",
    "from utils.dataloader import dataloader_vatexEnglish_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326a081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALOADER_DICT = {}\n",
    "DATALOADER_DICT[\"msrvtt\"] = {\"train\":dataloader_msrvtt_train, \"test\":dataloader_msrvtt_test}\n",
    "DATALOADER_DICT[\"msrvttfull\"] = {\"train\":dataloader_msrvtt_train, \"val\":dataloader_msrvttfull_test, \"test\":dataloader_msrvttfull_test}\n",
    "DATALOADER_DICT[\"msvd\"] = {\"train\":dataloader_msvd_train, \"val\":dataloader_msvd_test, \"test\":dataloader_msvd_test}\n",
    "DATALOADER_DICT[\"vatexEnglish\"] = {\"train\":dataloader_vatexEnglish_train, \"test\":dataloader_vatexEnglish_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae870286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed_logger(args):\n",
    "    \"\"\"Initialize the seed and environment variable\n",
    "\n",
    "    Args:\n",
    "        args: the hyper-parameters.\n",
    "\n",
    "    Returns:\n",
    "        args: the hyper-parameters modified by the random seed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global logger\n",
    "\n",
    "    # predefining random initial seeds\n",
    "    random.seed(args.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # get logger\n",
    "    logger = get_logger(os.path.join(args.output_dir))\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0682fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_device(args, local_rank):\n",
    "    \"\"\"Initialize device to determine CPU or GPU\n",
    "\n",
    "     Args:\n",
    "         args: the hyper-parameters\n",
    "         local_rank: GPU id\n",
    "\n",
    "     Returns:\n",
    "         devices: cuda\n",
    "         n_gpu: number of gpu\n",
    "\n",
    "     \"\"\"\n",
    "    global logger\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", local_rank)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))\n",
    "    args.n_gpu = n_gpu\n",
    "\n",
    "    if args.batch_size_val % args.n_gpu != 0:\n",
    "        raise ValueError(\"Invalid batch_size/batch_size_val and n_gpu parameter: {}%{} and {}%{}, should be == 0\".format(\n",
    "            args.batch_size, args.n_gpu, args.batch_size_val, args.n_gpu))\n",
    "\n",
    "    return device, n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9c6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_model(args, device):\n",
    "    \"\"\"Initialize model.\n",
    "\n",
    "    if location of args.init_model exists, model will be initialized from the pretrained model.\n",
    "    if no model exists, the training will be initialized from CLIP's parameters.\n",
    "\n",
    "    Args:\n",
    "        args: the hyper-parameters\n",
    "        devices: cuda\n",
    "\n",
    "    Returns:\n",
    "        model: the initialized model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # resume model if pre-trained model exist.\n",
    "    model_file = os.path.join(args.checkpoint, \"pytorch_model.bin.{}\".format(args.model_num))\n",
    "    if os.path.exists(model_file):\n",
    "        model_state_dict = torch.load(model_file, map_location='cpu')\n",
    "        if args.local_rank == 0:\n",
    "            logger.info(\"Model loaded from %s\", model_file)\n",
    "    else:\n",
    "        model_state_dict = None\n",
    "        if args.local_rank == 0:\n",
    "            logger.info(\"Model loaded fail %s\", model_file)\n",
    "\n",
    "    # Prepare model\n",
    "    model = CLIP2Video.from_pretrained(args.cross_model, cache_dir=None, state_dict=model_state_dict,\n",
    "                                       task_config=args)\n",
    "    model.to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1a9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "global logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4986478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6928097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(batch_size_val=64, cache_dir='', centerK=5, center_proj='TAB_TDB', center_type='TAB', center_weight=0.5, checkpoint='/share/home/lyq/CLIP2Video/CLIP2Video_MSVD', clip_path='/share/home/lyq/.cache/clip/ViT-B-32.pt', cross_model='cross-base', cross_num_hidden_layers=4, data_path='/share/home/lyq/Pretrain_attack/CLIP2Video/data/msvd_data/', datatype='msvd', do_eval=True, do_lower_case=False, feature_framerate=1, features_path='/share/test/lyq/video/test_MSVD', fp16=False, fp16_opt_level='O1', local_rank=0, max_frames=12, max_words=32, model_num='2', myreplace=None, n_gpu=1, num_thread_reader=4, output_dir='/share/home/lyq/Pretrain_attack/CLIP2Video/CLIP2Video_MSVD/try.txt', seed=42, sim_type='seqTransf', temporal_proj='sigmoid_selfA', temporal_type='TDB', val_csv='data/.val.csv', vocab_size=49408)\n"
     ]
    }
   ],
   "source": [
    "jupyter = True\n",
    "parser = argparse.ArgumentParser(description='CLIP2Video on Dideo-Text Retrieval Task')\n",
    "\n",
    "# arugment based on CLIP4clip:\n",
    "# https://github.com/ArrowLuo/CLIP4Clip/blob/668334707c493a4eaee7b4a03b2dae04915ce170/main_task_retrieval.py#L457\n",
    "parser.add_argument(\"--do_eval\", action='store_true', default=True, help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument('--val_csv', type=str, default='data/.val.csv', help='')\n",
    "parser.add_argument('--data_path', type=str, default='/share/home/lyq/Pretrain_attack/CLIP2Video/data/msvd_data/', help='data pickle file path')\n",
    "parser.add_argument('--features_path', type=str, default='/share/test/lyq/video/test_MSVD', help='feature path')\n",
    "parser.add_argument('--num_thread_reader', type=int, default=4, help='')\n",
    "parser.add_argument('--batch_size_val', type=int, default=64, help='batch size eval')\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed')\n",
    "parser.add_argument('--max_words', type=int, default=32, help='')\n",
    "parser.add_argument('--max_frames', type=int, default=12, help='')\n",
    "parser.add_argument('--feature_framerate', type=int, default=1, help='frame rate for uniformly sampling the video')\n",
    "parser.add_argument(\"--output_dir\", default='/share/home/lyq/Pretrain_attack/CLIP2Video/CLIP2Video_MSVD/try.txt', type=str,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument(\"--cross_model\", default=\"cross-base\", type=str, required=False, help=\"Cross module\")\n",
    "parser.add_argument(\"--do_lower_case\", action='store_true', help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument('--n_gpu', type=int, default=1, help=\"Changed in the execute process.\")\n",
    "parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
    "                    help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "parser.add_argument('--fp16', action='store_true',\n",
    "                    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
    "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                         \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "parser.add_argument('--cross_num_hidden_layers', type=int, default=4, help=\"Layer NO. of cross.\")\n",
    "\n",
    "\n",
    "# important extra argument for training and testing CLIP2Video\n",
    "parser.add_argument('--sim_type', type=str, default=\"seqTransf\", choices=[\"meanP\", \"seqTransf\"],\n",
    "                    help=\"choice a similarity header.\")\n",
    "\n",
    "# argument for testing\n",
    "parser.add_argument('--checkpoint', type=str, default='/share/home/lyq/CLIP2Video/CLIP2Video_MSVD', help=\"checkpoint dir\")\n",
    "parser.add_argument('--model_num', type=str, default='2', help=\"model id\")\n",
    "parser.add_argument('--local_rank', default=0, type=int, help='shard_id: node rank for distributed training')\n",
    "parser.add_argument(\"--datatype\", default=\"msvd\", type=str, help=\"msvd | msrvtt | vatexEnglish | msrvttfull\")\n",
    "\n",
    "# for different vocab size\n",
    "parser.add_argument('--vocab_size', type=int, default=49408, help=\"the number of vocab size\")\n",
    "\n",
    "# for TDB block\n",
    "parser.add_argument('--temporal_type', type=str, default='TDB', help=\"TDB type\")\n",
    "parser.add_argument('--temporal_proj', type=str, default='sigmoid_selfA', help=\"sigmoid_mlp | sigmoid_selfA\")\n",
    "\n",
    "# for TAB block\n",
    "parser.add_argument('--center_type', type=str, default='TAB', help=\"TAB\")\n",
    "parser.add_argument('--centerK', type=int, default=5, help='center number for clustering.')\n",
    "parser.add_argument('--center_weight', type=float, default=0.5, help='the weight to adopt the main simiarility')\n",
    "parser.add_argument('--center_proj', type=str, default='TAB_TDB', help='TAB | TAB_TDB')\n",
    "parser.add_argument('--myreplace', type=str, default=None)\n",
    "# model path of clip\n",
    "parser.add_argument('--clip_path', type=str,\n",
    "                    default='/share/home/lyq/.cache/clip/ViT-B-32.pt',\n",
    "                    help=\"model path of CLIP\")\n",
    "arg_list = None\n",
    "args = parser.parse_known_args()[0]\n",
    "print('args:', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc1cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = set_seed_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29684764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2022 18:38:50 - INFO -   device: cuda:0 n_gpu: 1\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu = init_device(args, args.local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79147f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ClipTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb3dfa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2022 18:39:04 - INFO -   Model loaded from /share/home/lyq/CLIP2Video/CLIP2Video_MSVD/pytorch_model.bin.2\n",
      "11/09/2022 18:39:05 - INFO -   loading archive file /share/home/lyq/Pretrain_attack/CLIP2Video/modules/cross-base\n",
      "11/09/2022 18:39:05 - INFO -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 77,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 512\n",
      "}\n",
      "\n",
      "11/09/2022 18:39:05 - INFO -   Weight doesn't exsits. /share/home/lyq/Pretrain_attack/CLIP2Video/modules/cross-base/cross_pytorch_model.bin\n",
      "11/09/2022 18:39:05 - WARNING -   \t embed_dim: 512\n",
      "11/09/2022 18:39:05 - WARNING -   \t image_resolution: 224\n",
      "11/09/2022 18:39:05 - WARNING -   \t vision_layers: 12\n",
      "11/09/2022 18:39:05 - WARNING -   \t vision_width: 768\n",
      "11/09/2022 18:39:05 - WARNING -   \t vision_patch_size: 32\n",
      "11/09/2022 18:39:05 - WARNING -   \t context_length: 77\n",
      "11/09/2022 18:39:05 - WARNING -   \t vocab_size: 49408\n",
      "11/09/2022 18:39:05 - WARNING -   \t transformer_width: 512\n",
      "11/09/2022 18:39:05 - WARNING -   \t transformer_heads: 8\n",
      "11/09/2022 18:39:05 - WARNING -   \t transformer_layers: 12\n",
      "11/09/2022 18:39:05 - WARNING -   \t cut_top_layer: 0\n",
      "11/09/2022 18:39:06 - WARNING -   \t sim_type: seqTransf\n",
      "11/09/2022 18:39:14 - INFO -   --------------------\n"
     ]
    }
   ],
   "source": [
    "model = init_model(args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "249c6585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test, sentence number: 27763\n",
      "For test, video number: 670\n",
      "Video number: 670\n",
      "Total Pair: 27763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/home/lyq/miniconda3/envs/lyq/lib/python3.6/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "assert args.datatype in DATALOADER_DICT\n",
    "test_dataloader, test_length = DATALOADER_DICT[args.datatype][\"test\"](args, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7c0c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2022 18:40:08 - INFO -   ***** Running test *****\n",
      "11/09/2022 18:40:08 - INFO -     Num examples = 27763\n",
      "11/09/2022 18:40:08 - INFO -     Batch size = 64\n",
      "11/09/2022 18:40:08 - INFO -     Num steps = 434\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == 0:\n",
    "        logger.info(\"***** Running test *****\")\n",
    "        logger.info(\"  Num examples = %d\", test_length)\n",
    "        logger.info(\"  Batch size = %d\", args.batch_size_val)\n",
    "        logger.info(\"  Num steps = %d\", len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2605b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from evaluation.metrics import tensor_text_to_video_metrics\n",
    "from evaluation.metrics import tensor_video_to_text_sim\n",
    "from utils.utils import parallel_apply\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def _run_on_single_gpu(model, batch_list_t, batch_list_v, batch_sequence_output_list, batch_visual_output_list):\n",
    "    \"\"\"run similarity in one single gpu\n",
    "    Args:\n",
    "        model: CLIP2Video\n",
    "        batch_list_t: id of text embedding\n",
    "        batch_list_v: id of visual embedding\n",
    "        batch_sequence_output_list: batch text embedding\n",
    "        batch_visual_output_list: batch visual embedding\n",
    "    Returns:\n",
    "        sim_matrix: similarity\n",
    "\n",
    "    \"\"\"\n",
    "    sim_matrix = []\n",
    "    for idx1, b1 in enumerate(batch_list_t):\n",
    "        input_mask, segment_ids, *_tmp = b1\n",
    "        sequence_output = batch_sequence_output_list[idx1]\n",
    "        each_row = []\n",
    "        for idx2, b2 in enumerate(batch_list_v):\n",
    "            video_mask, *_tmp = b2\n",
    "            visual_output = batch_visual_output_list[idx2]\n",
    "            # calculate the similarity\n",
    "            b1b2_logits, *_tmp = model.get_inference_logits(sequence_output, visual_output, input_mask, video_mask)\n",
    "            b1b2_logits = b1b2_logits.cpu().detach().numpy()\n",
    "            each_row.append(b1b2_logits)\n",
    "        each_row = np.concatenate(tuple(each_row), axis=-1)\n",
    "        sim_matrix.append(each_row)\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch_h(model, test_dataloader, device, n_gpu, logger):\n",
    "    \"\"\"run similarity in one single gpu\n",
    "    Args:\n",
    "        model: CLIP2Video\n",
    "        test_dataloader: data loader for test\n",
    "        device: device to run model\n",
    "        n_gpu: GPU number\n",
    "        batch_sequence_output_list: batch text embedding\n",
    "        batch_visual_output_list: batch visual embedding\n",
    "    Returns:\n",
    "        R1: rank 1 of text-to-video retrieval\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if hasattr(model, 'module'):\n",
    "        model = model.module.to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    # if multi_sentence_ == True: compute the similarity with multi-sentences retrieval\n",
    "    multi_sentence_ = False\n",
    "\n",
    "    cut_off_points_, sentence_num_, video_num_ = [], -1, -1\n",
    "    if hasattr(test_dataloader.dataset, 'multi_sentence_per_video') \\\n",
    "            and test_dataloader.dataset.multi_sentence_per_video:\n",
    "        multi_sentence_ = True\n",
    "        cut_off_points_ = test_dataloader.dataset.cut_off_points # used to tag the label when calculate the metric\n",
    "        sentence_num_ = test_dataloader.dataset.sentence_num # used to cut the sentence representation\n",
    "        video_num_ = test_dataloader.dataset.video_num # used to cut the video representation\n",
    "        cut_off_points_ = [itm - 1 for itm in cut_off_points_]\n",
    "\n",
    "    if multi_sentence_:\n",
    "        logger.warning(\"Eval under the multi-sentence per video clip setting.\")\n",
    "        logger.warning(\"sentence num: {}, video num: {}\".format(sentence_num_, video_num_))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_list_t = []\n",
    "        batch_list_v = []\n",
    "        batch_sequence_output_list, batch_visual_output_list = [], []\n",
    "        total_video_num = 0\n",
    "\n",
    "\n",
    "        for bid, batch in enumerate(test_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, video, video_mask = batch\n",
    "            # print(input_ids.shape)   #batch(64) * 32\n",
    "            # print(input_mask.shape)  #batch * 32\n",
    "            # print(segment_ids.shape)  #batch * 32\n",
    "            # print(video.shape)       #batch * 1 * frame * 1 * 3 * 224 * 224\n",
    "            # print(video_mask.shape)   #batch * 1 * frame\n",
    "            if multi_sentence_:\n",
    "                # multi-sentences retrieval means: one frame clip has two or more descriptions.\n",
    "                b, *_t = video.shape\n",
    "                sequence_output = model.get_sequence_output(input_ids, segment_ids, input_mask)\n",
    "                batch_sequence_output_list.append(sequence_output)\n",
    "                batch_list_t.append((input_mask, segment_ids,))\n",
    "\n",
    "                s_, e_ = total_video_num, total_video_num + b\n",
    "                # print(cut_off_points_)\n",
    "                filter_inds = [itm - s_ for itm in cut_off_points_ if itm >= s_ and itm < e_]\n",
    "                # print(len(batch_sequence_output_list))\n",
    "\n",
    "                if len(filter_inds) > 0:\n",
    "                    video, video_mask = video[filter_inds, ...], video_mask[filter_inds, ...]\n",
    "                    visual_output = model.get_visual_output(video, video_mask)\n",
    "                    batch_visual_output_list.append(visual_output)\n",
    "                    batch_list_v.append((video_mask,))\n",
    "                # print(len(batch_visual_output_list))\n",
    "                total_video_num += b\n",
    "            else:\n",
    "                sequence_output, visual_output = model.get_sequence_visual_output(input_ids, segment_ids, input_mask, video, video_mask)\n",
    "\n",
    "                batch_sequence_output_list.append(sequence_output)\n",
    "                batch_list_t.append((input_mask, segment_ids,))\n",
    "\n",
    "                batch_visual_output_list.append(visual_output)\n",
    "                batch_list_v.append((video_mask,))\n",
    "\n",
    "            #print(\"{}/{}\\r\".format(bid, len(test_dataloader)), end=\"\")\n",
    "            if bid % 50 == 0:\n",
    "                print(\"now: \", bid)\n",
    "        return model, batch_list_t, batch_list_v, batch_sequence_output_list, batch_visual_output_list\n",
    "       \n",
    "        # calculate the similarity  in one GPU\n",
    "#         sim_matrix = _run_on_single_gpu(model, batch_list_t, batch_list_v, batch_sequence_output_list, batch_visual_output_list)\n",
    "#         sim_matrix = np.concatenate(tuple(sim_matrix), axis=0)\n",
    "\n",
    "#         R1 = logging_rank(sim_matrix, multi_sentence_, cut_off_points_, logger)\n",
    "#         return R1\n",
    "\n",
    "def logging_rank(sim_matrix, multi_sentence_, cut_off_points_, logger):\n",
    "    \"\"\"run similarity in one single gpu\n",
    "    Args:\n",
    "        sim_matrix: similarity matrix\n",
    "        multi_sentence_: indicate whether the multi sentence retrieval\n",
    "        cut_off_points_:  tag the label when calculate the metric\n",
    "        logger: logger for metric\n",
    "    Returns:\n",
    "        R1: rank 1 of text-to-video retrieval\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if multi_sentence_:\n",
    "        # if adopting multi-sequence retrieval, the similarity matrix should be reshaped\n",
    "        logger.info(\"before reshape, sim matrix size: {} x {}\".format(sim_matrix.shape[0], sim_matrix.shape[1]))\n",
    "        cut_off_points2len_ = [itm + 1 for itm in cut_off_points_]\n",
    "        max_length = max([e_-s_ for s_, e_ in zip([0]+cut_off_points2len_[:-1], cut_off_points2len_)])\n",
    "        sim_matrix_new = []\n",
    "        for s_, e_ in zip([0] + cut_off_points2len_[:-1], cut_off_points2len_):\n",
    "            sim_matrix_new.append(np.concatenate((sim_matrix[s_:e_],\n",
    "                                                  np.full((max_length-e_+s_, sim_matrix.shape[1]), -np.inf)), axis=0))\n",
    "        sim_matrix = np.stack(tuple(sim_matrix_new), axis=0)   # 670 x 81(max_length) x 670\n",
    "        logger.info(\"after reshape, sim matrix size: {} x {} x {}\".\n",
    "                    format(sim_matrix.shape[0], sim_matrix.shape[1], sim_matrix.shape[2]))\n",
    "\n",
    "        # compute text-to-video retrieval\n",
    "        tv_metrics = tensor_text_to_video_metrics(sim_matrix)\n",
    "\n",
    "\n",
    "        # compute video-to-text retrieval\n",
    "        tmp = tensor_video_to_text_sim(sim_matrix)\n",
    "\n",
    "        print(tmp.shape)  ## debug\n",
    "        vt_metrics = compute_metrics(tmp)\n",
    "    else:\n",
    "        logger.info(\"sim matrix size: {}, {}\".format(sim_matrix.shape[0], sim_matrix.shape[1]))\n",
    "\n",
    "        # compute text-to-video retrieval\n",
    "        tv_metrics = compute_metrics(sim_matrix)\n",
    "\n",
    "        # compute video-to-text retrieval\n",
    "        vt_metrics = compute_metrics(sim_matrix.T)\n",
    "        logger.info('\\t Length-T: {}, Length-V:{}'.format(len(sim_matrix), len(sim_matrix[0])))\n",
    "\n",
    "\n",
    "    # logging the result of text-to-video retrieval\n",
    "    logger.info(\"Text-to-Video:\")\n",
    "    logger.info('\\t>>>  R@1: {:.1f} - R@5: {:.1f} - R@10: {:.1f} - Median R: {:.1f} - Mean R: {:.1f}'.\n",
    "                format(tv_metrics['R1'], tv_metrics['R5'], tv_metrics['R10'], tv_metrics['MR'], tv_metrics['MeanR']))\n",
    "\n",
    "    # logging the result of video-to-text retrieval\n",
    "    logger.info(\"Video-to-Text:\")\n",
    "    logger.info(\n",
    "        '\\t>>>  V2T$R@1: {:.1f} - V2T$R@5: {:.1f} - V2T$R@10: {:.1f} - V2T$Median R: {:.1f} - V2T$Mean R: {:.1f}'.format(\n",
    "            vt_metrics['R1'], vt_metrics['R5'], vt_metrics['R10'], vt_metrics['MR'], vt_metrics['MeanR']))\n",
    "\n",
    "    R1 = tv_metrics['R1']\n",
    "    return R1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b738f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2022 18:46:47 - WARNING -   Eval under the multi-sentence per video clip setting.\n",
      "11/09/2022 18:46:47 - WARNING -   sentence num: 27763, video num: 670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now:  0\n",
      "now:  50\n",
      "now:  100\n",
      "now:  150\n",
      "now:  200\n",
      "now:  250\n",
      "now:  300\n",
      "now:  350\n",
      "now:  400\n"
     ]
    }
   ],
   "source": [
    "model, batch_list_t, batch_list_v, batch_sequence_output_list, batch_visual_output_list = eval_epoch_h(model, test_dataloader, device, n_gpu, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c540b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "434\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_list_v))\n",
    "print(len(batch_list_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7ad3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = _run_on_single_gpu(model, batch_list_t, batch_list_v, batch_sequence_output_list, batch_visual_output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f56b472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b849e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n"
     ]
    }
   ],
   "source": [
    "print(len(sim_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01867f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.concatenate(tuple(sim_matrix), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91c625d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27763, 670)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4bfaa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac1b8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_points_, sentence_num_, video_num_ = [], -1, -1\n",
    "if hasattr(test_dataloader.dataset, 'multi_sentence_per_video') \\\n",
    "        and test_dataloader.dataset.multi_sentence_per_video:\n",
    "    multi_sentence_ = True\n",
    "    cut_off_points_ = test_dataloader.dataset.cut_off_points # used to tag the label when calculate the metric\n",
    "    sentence_num_ = test_dataloader.dataset.sentence_num # used to cut the sentence representation\n",
    "    video_num_ = test_dataloader.dataset.video_num # used to cut the video representation\n",
    "    cut_off_points_ = [itm - 1 for itm in cut_off_points_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66974c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n"
     ]
    }
   ],
   "source": [
    "print(len(cut_off_points_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6f947d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2022 19:55:02 - INFO -   before reshape, sim matrix size: 27763 x 670\n",
      "11/09/2022 19:55:03 - INFO -   after reshape, sim matrix size: 670 x 81 x 670\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"before reshape, sim matrix size: {} x {}\".format(sim_matrix.shape[0], sim_matrix.shape[1]))\n",
    "cut_off_points2len_ = [itm + 1 for itm in cut_off_points_]\n",
    "max_length = max([e_-s_ for s_, e_ in zip([0]+cut_off_points2len_[:-1], cut_off_points2len_)])\n",
    "sim_matrix_new = []\n",
    "for s_, e_ in zip([0] + cut_off_points2len_[:-1], cut_off_points2len_):\n",
    "    sim_matrix_new.append(np.concatenate((sim_matrix[s_:e_],\n",
    "                                          np.full((max_length-e_+s_, sim_matrix.shape[1]), -np.inf)), axis=0))\n",
    "sim_matrix = np.stack(tuple(sim_matrix_new), axis=0)   # 670 x 81(max_length) x 670\n",
    "logger.info(\"after reshape, sim matrix size: {} x {} x {}\".\n",
    "            format(sim_matrix.shape[0], sim_matrix.shape[1], sim_matrix.shape[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "211946e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_metrics = tensor_text_to_video_metrics(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c7eb330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 81, 670)\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbbd8ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([670, 670])\n"
     ]
    }
   ],
   "source": [
    "tmp = tensor_video_to_text_sim(sim_matrix)\n",
    "\n",
    "print(tmp.shape)  ## debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f006cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x):\n",
    "\n",
    "    print(\"x\", x.shape) #debug\n",
    "    sx = np.sort(-x, axis=1)\n",
    "    print(\"sx\", sx.shape) #debug\n",
    "    d = np.diag(-x)\n",
    "    d = d[:, np.newaxis]\n",
    "    print(\"d\", d.shape) #debug\n",
    "    ind = sx - d\n",
    "    print(\"ind\", ind.shape) #debug\n",
    "    print(\"sum\", np.sum(ind==0))\n",
    "    ind = np.where(ind == 0)\n",
    "    print(ind)\n",
    "    ind = ind[1]\n",
    "    \n",
    "#     with open(\"correct.txt\", \"a+\") as fp:\n",
    "#         np.savetxt(fp, ind, fmt='%d', delimiter=',')\n",
    "    # b = np.loadtxt(filename, dtype=np.int32, delimiter=',')\n",
    "    metrics = {}\n",
    "    metrics['R1'] = float(np.sum(ind == 0)) * 100 / len(ind)\n",
    "    metrics['R5'] = float(np.sum(ind < 5)) * 100 / len(ind)\n",
    "    metrics['R10'] = float(np.sum(ind < 10)) * 100 / len(ind)\n",
    "    metrics['MR'] = np.median(ind) + 1\n",
    "    metrics[\"MedianR\"] = metrics['MR']\n",
    "    metrics[\"MeanR\"] = np.mean(ind) + 1\n",
    "    metrics[\"cols\"] = [int(i) for i in list(ind)]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47ddf635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([670, 670])\n",
      "sx (670, 670)\n",
      "d (670, 1)\n",
      "ind (670, 670)\n",
      "sum 714\n",
      "714\n"
     ]
    }
   ],
   "source": [
    "vt_metrics = compute_metrics(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d01490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
